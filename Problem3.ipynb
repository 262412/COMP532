{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134c952a",
   "metadata": {},
   "source": [
    "1. Introduction and Background (Problem 1 & 2)\n",
    "\n",
    "The Multi-armed Bandit (MAB) problem is a classic framework in Reinforcement Learning that exemplifies the exploration-exploitation trade-off. In this experiment, we simulate a k-armed bandit environment where each action (pulling a lever) provides a reward sampled from a normal distribution. The goal is to maximize the cumulative reward over 2000 steps.\n",
    "1.1 Algorithm Design: ϵ-Greedy Strategy\n",
    "\n",
    "To solve this, we implemented the ϵ-greedy algorithm. This strategy balances two conflicting objectives:\n",
    "\n",
    "    Exploitation: With probability 1−ϵ, the agent chooses the action with the highest estimated value to maximize immediate reward.\n",
    "\n",
    "    Exploration: With probability ϵ, the agent chooses an action at random to discover potentially better options that are currently undervalued.\n",
    "\n",
    "1.2 Experimental Setup\n",
    "\n",
    "    Arm Count (k): Tested for k=5,10,20.\n",
    "\n",
    "    Reward Distribution: True values q∗​(a) are sampled from N(0,1). The actual reward received is N(q∗​(a),1).\n",
    "\n",
    "    Parameters: ϵ=[0,0.01,0.1].\n",
    "\n",
    "    Trials: Results are averaged over 2000 independent tasks to ensure statistical significance.\n",
    "\n",
    "\n",
    "2. Action-Value Methods (Problem 3)\n",
    "2.1 Theoretical Framework\n",
    "\n",
    "Action-value methods focus on estimating the value of actions and using these estimates to make decisions. The true value of an action a, denoted as q∗​(a), is the expected reward:\n",
    "q∗​(a)=E[Rt​∣At​=a]\n",
    "\n",
    "We use the Sample-Average Method to estimate these values. The estimate Qt​(a) is the average of the rewards received for action a prior to time t:\n",
    "Qt​(a)=Number of times action a was takenSum of rewards when action a was taken​\n",
    "2.2 Incremental Implementation\n",
    "\n",
    "In our code (specifically the update_estimate function), we implement an incremental update rule to avoid storing the full history of rewards, which optimizes memory usage:\n",
    "Qn+1​=Qn​+n1​[Rn​−Qn​]\n",
    "\n",
    "\n",
    "Here, Rn​ is the reward received after the n-th selection of action a, and [Rn​−Qn​] represents the estimation error. As n increases, the error's impact diminishes, and Q converges to the true mean.\n",
    "3. Results and Discussion\n",
    "3.1 Performance Visualization\n",
    "\n",
    "Below are the results generated by our implementation (please insert your figure_k10.png here):\n",
    "3.2 Analysis of Results\n",
    "\n",
    "    The Greedy Trap (ϵ=0):\n",
    "    As shown in the \"Average Reward\" plots, the greedy method (ϵ=0) improves slightly at the very beginning but quickly plateaus at a low level. This is because it often gets stuck performing a sub-optimal action that happened to give a positive reward early on, never exploring the other arms to find the true optimal one.\n",
    "\n",
    "    The Effect of ϵ values:\n",
    "\n",
    "        ϵ=0.1: This setting explores more aggressively. It finds the optimal action much faster than other methods. However, in the long run, its average reward is capped because it continues to choose sub-optimal random actions 10% of the time.\n",
    "\n",
    "        ϵ=0.01: This method improves more slowly but eventually outperforms ϵ=0.1 in both reward and optimal action percentage. It takes longer to identify the best arm, but once found, it exploits it 99% of the time.\n",
    "\n",
    "    Scalability with k:\n",
    "    Comparing k=5 and k=20, we observe that as the number of arms increases, the \"Optimal Action %\" curve rises more slowly. This indicates that in a larger action space, more exploration is required to build reliable action-value estimates.\n",
    "\n",
    "4. Conclusion\n",
    "\n",
    "The experiment demonstrates that Action-Value Methods combined with an ϵ-greedy policy are effective for solving the MAB problem. The incremental update rule provides an efficient way to track action values. While the greedy approach fails due to a lack of exploration, the ϵ-greedy approach successfully navigates the exploration-exploitation trade-off, with ϵ=0.1 being better for short-term learning and ϵ=0.01 being superior for long-term stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
